~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dataset Generation Protocol
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Parameter Selection
   - Primary control parameters: theta_cap, kernel_size, SNR
   - Each parameter has 3 difficulty levels: hard, medium, easy
     * theta_cap: [1e-3, 1e-4, 1e-5] (dense → sparse activation)
     * kernel_size: [0.16, 0.04, 0.01] (ratio of image area)
     * SNR: [1.0, 3.16, 10.0] (low → high signal quality)

2. Sampling Strategy
   Using "targeted sparse" sampling to cover key regions:
   a) Corner cases:
      - Hardest case: All parameters at hardest setting
      - Easiest case: All parameters at easiest setting
   
   b) Single-parameter stress tests:
      - Hard theta_cap with easy other parameters
      - Hard kernel_size with easy other parameters
      - Hard SNR with easy other parameters
   
   c) Medium-difficulty combinations:
      - All medium parameters
      - Various Medium-Easy combinations to explore transition regions

3. Dataset Generation Protocol
   - Fixed parameters:
     * image_size: [500, 500]
     * num_kernels: 2
     * n (energy layers): 1
     * relative parameters: equal for all kernels
   
   - Interactive validation:
     * Visual inspection of each generated dataset
     * Option to regenerate if results are unsatisfactory
     * Confirmation required before proceeding

4. Data Collection and Storage
   - Save complete parameter sets
   - Store both clean and noisy observations
   - Preserve ground truth kernels and activations
   - Include timestamp in saved files for tracking
   - Maintain parameter descriptions for analysis

This systematic approach allows for:
- Comprehensive coverage of the parameter space
- Focus on boundary conditions and transition regions
- Quality control through visual inspection
- Reproducible dataset generation
- Traceable parameter combinations for analysis

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Parallel Testing Strategy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Algorithm Parameter Space
   - Key tunable parameters:
     * lambda1: [1e-2, 3.16e-2, 1e-1] (regularization strength)
     * mini_loop: [1, 3, 9] (inner iteration count)
   - Fixed parameters:
     * maxIT: 30 (maximum outer iterations)
     * phase2: false
     * Xsolve: 'FISTA'
     * xpos: true
     * getbias: true

2. Testing Infrastructure
   a) Parallel Processing:
      - Utilizes MATLAB's parallel computing toolbox
      - Default pool size: 9 workers
      - Each worker handles different parameter combinations
   
   b) Configuration Management:
      - Separate config directories for each parameter combination
      - Dynamic config file generation and cleanup
      - Isolated worker environments to prevent interference

3. Test Execution Protocol
   - For each dataset:
     * Initialize kernels consistently
     * Broadcast dataset parameters to workers
     * Run all parameter combinations in parallel
     * Collect and store results including:
       - Output kernels (Aout)
       - Activation maps (Xout)
       - Bias terms (bout)
       - Additional metrics (extras)
       - Success/failure status
       - Error tracking

4. Results Collection and Storage
   - Comprehensive result structure including:
     * All algorithm outputs
     * Parameter combinations used
     * Success/failure flags
     * Error messages for failed cases
   - Timestamp-based file naming
   - Uses -v7.3 MAT-file format for large datasets

5. Error Handling
   - Robust error catching for each parameter combination
   - Continues execution even if individual tests fail
   - Preserves error information for failed cases
   - Maintains overall test stability

This parallel testing approach enables:
- Efficient exploration of algorithm parameter space
- Systematic evaluation of parameter combinations
- Robust handling of computational resources
- Comprehensive result collection and analysis
- Reproducible testing environment

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Evaluation Metrics and Analysis Strategy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1. Iteration-Level Metrics
   a) Phase I Metrics (tracked per iteration):
      - Activation Recovery Accuracy:
        * Per-kernel activation similarity with ground truth
        * Tracked across all iterations
        * Stored in extras.phase1.activation_metrics[iter, kernel]
      
      - Kernel Quality Factors:
        * Similarity between recovered and true kernels
        * Tracked for each kernel independently
        * Stored in extras.phase1.kernel_quality_factors[iter, kernel]

   b) Phase II Metrics (if enabled):
      - Refinement Quality Tracking:
        * Activation metrics across refinement steps
        * Kernel quality evolution during sphere lifting
        * Stored in extras.phase2.activation_metrics[refinement, kernel]
        * Stored in extras.phase2.kernel_quality_factors[refinement, kernel]

2. Performance Metrics
   - Runtime Analysis:
      * Per-iteration timing
      * Total execution time
      * Stored in extras.runtime
   
   - Convergence Indicators:
      * Kernel normalization factors (extras.normA)
      * Final bias terms (bout)
      * Parameter-specific information (extras.parameters)

3. Quality Assessment Framework
   - Multi-dimensional evaluation:
      * Activation map accuracy
      * Kernel reconstruction quality
      * Computational efficiency
      * Convergence stability
   
   - Cross-parameter Analysis:
      * Impact of lambda1 on convergence
      * Effect of mini_loop on solution quality
      * Parameter sensitivity analysis
      * Optimal parameter region identification

4. Comparative Analysis Tools
   - Parameter Space Exploration:
      * Performance variation across parameter combinations
      * Identification of stable operating regions
      * Trade-off analysis between speed and accuracy
   
   - Robustness Assessment:
      * Performance consistency across different initializations
      * Sensitivity to noise levels
      * Stability across different kernel configurations

This comprehensive metrics framework enables:
- Quantitative assessment of algorithm performance
- Systematic parameter optimization
- Identification of convergence patterns
- Robust comparison across different test conditions
- Data-driven refinement of algorithm parameters




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Visualization Strategy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Goal of visualization:
- visualize the performance of the algorithm across the system parameter space
- To determine the optimal system parameter setting for the algorithm, or if the optimal setting is different for different datasets
   - if the optimal setting is different for different datasets, we need to visualize the performance difference across different datasets,
     and see if there is any correlation between the system parameters and the performance.
- Therefore, our first step is to visualize the performance of the algorithm across the system parameter space.

Questions: 
1. what are we visualizing? 
   - the metrics we defined in the evaluation metrics and analysis strategy section, including:
     * activation recovery accuracy
     * kernel quality factors
     * runtime analysis
     * convergence indicators-> Did the algorithm converge? If yes, how well the algorithm converges, and if convergence is achieved, how fast the algorithm converges within maxIT=30

2. how are we visualizing? 
   -  In total, there are 2 system parameters: lambda1 and mini_loop, and each has 3 levels: [1e-2, 3.16e-2, 1e-1] and [1, 3, 9] respectively.
      - therefore, we are visualizing the performance of the algorithm across the system parameter space, which is a 2D space.
      - since there are only 9 points, we can use a heatmap. 
      - the x-axis is lambda1, the y-axis is mini_loop, and the z-axis is the metrics we defined in the evaluation metrics and analysis strategy section.
      - each metrics will have a heatmap, and in total there will be 4 heatmaps.
